<p align="center">
  <h1 align="center">MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs</h1>
    <p align="center">
  üìñ<a href="https://arxiv.org/submit/6677689/view">Paper</a> |
  ü§ó<a href="https://huggingface.co/datasets/guojianz/MolReasoner">Datasets</a> | ü§ñ<a 
  href="https://modelscope.cn/models/cody545487677/MolReasoner/">Models Weights</a></h3>
<div align="center"></div>
<p align="center">
  <p>

üåàLarge Language Models (LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce **MolReasoner**, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the model‚Äôs reasoning abilities via synthetic Chainof-Thought (CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model‚Äôs molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning.

## üì¢ News

<!-- - üöÄ [08/04/2025] Our paper **Visual-RFT** is accepted by ICCV 2025!
- üöÄ [05/21/2025] We support both **HuggingFace Dataset** format and **JSON** file format as input datasets for training.
- üöÄ [05/21/2025] We updata the trainer of **Visual-RFT** to support both Qwen2-VL and Qwen2.5-VL. And we support multi-image inputs with `grpo_trainer_mp.py`.
- üöÄ [05/20/2025] We release **Visual-ARFT** repository <a href="https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT">Repo-URL</a>: A RFT framework dedicated to enhancing the **multimodal agentic capabilities of LVLMs**. (Support Qwen2-VL and Qwen2.5-VL)
- üöÄ [03/12/2025] We release the code of **Visual-RFT** to build the <a href="https://github.com/Liuziyu77/Visual-RFT/tree/main/dataset">dataset</a> on your own data.
- üöÄ [03/04/2025] We release our **Visual-RFT's** <a href="https://arxiv.org/abs/2503.01785">Paper</a>. -->

- üöÄ [08/04/2025] We upload our checkpoints of **MolReasoner** to <a href="https://modelscope.cn/models/cody545487677/MolReasoner/">ModelScope</a>.
- üöÄ [08/04/2025] We upload our training datasets of **MolReasoner** to <a href="https://huggingface.co/datasets/guojianz/MolReasoner">Huggingface</a>.
- üöÄ [08/04/2025] We release **MolReasoner** repository and our training, inference and evaluation code.

## üõ†Ô∏è Setup

```
# 1. Install LLaMA-Factory from https://github.com/hiyouga/LLaMA-Factory repository.
# 2. Install Verl from https://github.com/volcengine/verl repository.
# 3. Install additional dependencies for both environments
pip3 install deepspeed
pip install --force-reinstall psutil==5.9.8
pip install -U "ray[data,train,tune,serve,default]"
pip install EFGs
pip install swanlab
pip install --upgrade boto3 botocore
pip install rdkit tensorboard
pip install python-Levenshtein
pip install selfies
pip install nltk

# 4. Configure NLTK data path
cp -r verl/nltk_data /root/nltk_data

# 5. Download the SciBERT model
# Download the SciBERT model from Hugging Face:
# https://huggingface.co/allenai/scibert_scivocab_uncased or
# https://huggingface.co/Sihangli/3D-MoLM


# 6. Download the QWEN 2.5-7B-Instruct model
# Download the QWEN 2.5-7B-Instruct model from Hugging Face:
# https://huggingface.co/Qwen/Qwen2.5-7B-Instruct

```

## Mol-SFT

### Data Preparation:

Please download the data from Hugging Face and place the corresponding SFT data under the `LLaMA-Factory/data` directory. Then, store the data according to the information in `LLaMA-Factory/data/dataset_info.json` as follows:

```json
{
  "text_based_de_novo_molecule_generation_train": {
    "file_name": "text_based_de_novo_molecule_generation_train.json"
  },
  "text_based_de_novo_molecule_generation_test": {
    "file_name": "text_based_de_novo_molecule_generation_test.json"
  },
  "molecule_captioning_train": {
    "file_name": "molecule_captioning_train.json"
  },
  "molecule_captioning_test": {
    "file_name": "molecule_captioning_test.json"
  }
}
```

### Model Training

```bash
# 1. Molecule Captioning
bash LLaMA-Factory/train_molecule_captioning.sh
# 2. Text-based De Novo Molecule Generation
bash LLaMA-Factory/train_text_based_de_novo_molecule_generation.sh
```

Please remember to update the base model paths (`Qwen2.5-7B-Instruct`) in the following YAML files:

- `LLaMA-Factory/examples/train_full/train_molecule_captioning/sft.yml`
- `LLaMA-Factory/examples/train_full/train_text_guided_molecule_generation/sft.yml`

Make sure to modify the paths to the downloaded model and adjust the save paths as needed.

## Mol-RL

### Data Preparation:

Please download the grpo data from Huggingface.

### Model Training

```bash
# 1. Molecule Captioning
bash verl/examples/grpo_trainer/grpo_train_molecule_captioning.sh
# 2. Text-based De Novo Molecule Generation
bash verl/examples/grpo_trainer/grpo_train_text_based_de_novo_molecule_generation.sh
```

Please make sure to follow the notes provided in the two shell files and update the paths accordingly.

Additionally, during the Molecule Captioning training, make sure to replace the line `primary_path = 'xxxx/scibert_scivocab_uncased'` in the file `verl/verl/utils/reward_score/chembl_mol2desc.py` with your own model path.

For convenience, in the code:

- `desc2mol` is set to refer to `text_guided_molecule_generation`, and
- `mol2desc` is set to refer to `molecule_captioning`.

### Merge Model

Please refer to `verl/scripts/model_merger.sh` to merge the trained actor model with the `Qwen2.5-7B-Instruct` model.

### Inference

Please refer to `LLaMA-Factory/infer_molecule_captioning.sh` and `LLaMA-Factory/infer_text_based_de_novo_molecule_generation.sh`. Make sure to replace the paths to the merged model and update the output paths for inference.

## Evaluation

We have provided scripts to evaluate both tasks, located at:

- `verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/eval_metrics.py`
- `verl/examples/data_preprocess/molecule/text_guided_molecule_generation/eval_text_guided_molecule_generation/eval_metrics.py`

Additionally, to demonstrate the effectiveness, we have included several baseline examples as well as the metrics from our method in the following directories:

- `verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/saved_results`

- `verl/examples/data_preprocess/molecule/text_guided_molecule_generation/eval_text_guided_molecule_generation/saved_results`

**To perform the metric evaluation, we have also provided some baseline evaluation scripts. The results here are exactly the same as those presented in our paper.**

```bash
python verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/grpo_eval.py # MolReasoner Molecule Captioning Evaluation
python verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/eval_mol_instruct.py # Mol-Instruction Molecule Captioning Evaluation
python verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/eval_qwen2_5_7b.py # Qwen2.5-7B Molecule Captioning Evaluation
python verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/eval_llama3_70b.py # Llama3-70B Molecule Captioning Evaluation
python verl/examples/data_preprocess/molecule/text_guided_molecule_generation/eval_text_guided_molecule_generation/grpo_eval.py # MolReasoner Text-based De Novo Molecule Generation Evaluation
```

## ‚úíÔ∏èCitation

```
TODO
```

## Acknowledgement

We sincerely thank projects <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>, <a href="https://github.com/volcengine/verl">Verl</a>, <a href="https://github.com/zjunlp/Mol-Instructions">Mol-Instructions</a>, and <a href="https://github.com/Liuziyu77/Visual-RFT/blob/main">Visual-RFT</a> for providing their open-source resources.

## ‚≠ê Star History

[![Star History Chart](https://api.star-history.com/svg?repos=545487677/MolReasoner&type=Date)](https://www.star-history.com/#545487677/MolReasoner&Date)
