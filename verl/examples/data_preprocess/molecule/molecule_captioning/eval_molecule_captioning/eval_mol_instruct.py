
import json
import pandas as pd

import pandas as pd
import json
import re
import numpy as np
np.random.seed(42)
from tqdm import tqdm
import argparse
import csv
from transformers import BertTokenizerFast
from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer

def extract_solution(solution_str: str) -> str:
    answer_pattern = r'<answer>(.*?)</answer>'
    matches = list(re.finditer(answer_pattern, solution_str, re.DOTALL))
    if matches:
        return matches[-1].group(1).strip()
    return None

def evaluate_text_metrics(data, text_model='/fs_mol/guojianz/projects/FunMG/LLM/model_weight/models--Sihangli--3D-MoLM/scibert_scivocab_uncased', text_trunc_length=2048):
    outputs = []
    for i, row in tqdm(data.iterrows(), total=len(data)):
        outputs.append(( row['ground truth'], row['output']))
    text_tokenizer = BertTokenizerFast.from_pretrained(text_model)
    bleu_scores = []
    meteor_scores = []

    references = []
    hypotheses = []
    for i, (gt, out) in enumerate(outputs):

        if i % 100 == 0: print(i, 'processed.')

        gt_tokens = text_tokenizer.tokenize(gt, truncation=True, max_length=text_trunc_length,
                                            padding='max_length')
        gt_tokens = list(filter(('[PAD]').__ne__, gt_tokens))
        gt_tokens = list(filter(('[CLS]').__ne__, gt_tokens))
        gt_tokens = list(filter(('[SEP]').__ne__, gt_tokens))

        out_tokens = text_tokenizer.tokenize(out, truncation=True, max_length=text_trunc_length,
                                            padding='max_length')
        out_tokens = list(filter(('[PAD]').__ne__, out_tokens))
        out_tokens = list(filter(('[CLS]').__ne__, out_tokens))
        out_tokens = list(filter(('[SEP]').__ne__, out_tokens))
        if out == '':
            print('output is empty, fill ""')

        references.append([gt_tokens])
        hypotheses.append(out_tokens)

        mscore = meteor_score([gt_tokens], out_tokens)
        meteor_scores.append(mscore)

    bleu2 = corpus_bleu(references, hypotheses, weights=(.5,.5))
    bleu4 = corpus_bleu(references, hypotheses, weights=(.25,.25,.25,.25))

    print('BLEU-2 score:', bleu2)
    print('BLEU-4 score:', bleu4)
    _meteor_score = np.mean(meteor_scores)
    print('Average Meteor score:', _meteor_score)

    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])

    rouge_scores = []

    references = []
    hypotheses = []

    for i, (gt, out) in enumerate(outputs):

        rs = scorer.score(out, gt)
        rouge_scores.append(rs)

    print('ROUGE score:')
    rouge_1 = np.mean([rs['rouge1'].fmeasure for rs in rouge_scores])
    rouge_2 = np.mean([rs['rouge2'].fmeasure for rs in rouge_scores])
    rouge_l = np.mean([rs['rougeL'].fmeasure for rs in rouge_scores])
    print('rouge1:', rouge_1)
    print('rouge2:', rouge_2)
    print('rougeL:', rouge_l)
    return bleu2, bleu4, rouge_1, rouge_2, rouge_l, _meteor_score


def evaluate_from_json(json_path=None, text_model='/fs_mol/guojianz/projects/FunMG/LLM/model_weight/models--Sihangli--3D-MoLM/scibert_scivocab_uncased'):
    with open(json_path, 'r', encoding='utf-8') as f:
        records = [json.loads(line) for line in f if line.strip()]
    data = pd.DataFrame(records)
    data['predict_extrac'] = data['predict']
    data['output'] = data['predict_extrac'].apply(lambda x: x.rsplit('.', 1)[0] + '.' if isinstance(x, str) else x)
    data['output'] = data['output'].fillna('')
    num_none = data['output'].isna().sum()
    print(f"Number of None in predict: {num_none}")
    data['ground truth'] = data['label']
    text_tokenizer = BertTokenizerFast.from_pretrained(text_model)
    data['output_token_length'] = data['output'].apply(
        lambda x: len(text_tokenizer.tokenize(x)) if isinstance(x, str) else 0
    )

    print(data['output_token_length'].describe())
    bleu2, bleu4, rouge_1, rouge_2, rouge_l, _meteor_score = evaluate_text_metrics(data)
    return {
        "bleu2": bleu2,
        "bleu4": bleu4,
        "meteor": _meteor_score,
        "rouge1": rouge_1,
        "rouge2": rouge_2,
        "rougeL": rouge_l,
    }



def evaluate(json_path: str, output_txt_path: str):
    with open(json_path, 'r', encoding='utf-8') as f:
            records = [json.loads(line) for line in f if line.strip()]

    data = pd.DataFrame(records)
    metrics = evaluate_from_json(json_path=json_path)
    with open(output_txt_path, "w") as f:
        f.write("ðŸ”¬ Evaluation Metrics (Text-Based)\n")
        f.write("=====================\n")
        f.write(f"ðŸ“˜ BLEU-2 (â†‘): {metrics['bleu2']:.4f}\n")
        f.write(f"ðŸ“˜ BLEU-4 (â†‘): {metrics['bleu4']:.4f}\n")
        f.write(f"ðŸ’¡ METEOR (â†‘): {metrics['meteor']:.4f}\n")
        f.write(f"ðŸ“• ROUGE-1 (â†‘): {metrics['rouge1']:.4f}\n")
        f.write(f"ðŸ“• ROUGE-2 (â†‘): {metrics['rouge2']:.4f}\n")
        f.write(f"ðŸ“• ROUGE-L (â†‘): {metrics['rougeL']:.4f}\n")

if __name__ == "__main__":
    import csv
    import json

    # print(f"âœ… JSON saved to: {output_file}")
    evaluate(
        json_path='xxx/MolReasoner/verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/saved_results/mol_instruct.jsonl',
        output_txt_path="xxxx/MolReasoner/verl/examples/data_preprocess/molecule/molecule_captioning/eval_molecule_captioning/saved_results/mol_instruct_metrics.txt"
    )


# BLEU-2 score: 0.09561133099215179
# BLEU-4 score: 0.06668488893115516
# Average Meteor score: 0.1890869090157156
# ROUGE score:
# rouge1: 0.28008794309781043
# rouge2: 0.1823275583746182
# rougeL: 0.25820713015778174